{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Key Components to Implement**\n",
    "1. **Recurrent Neural Networks (RNNs)**:\n",
    "   - Two layers: Sensory layer and M1 layer.\n",
    "   - Sensory layer processes proprioceptive feedback.\n",
    "   - M1 layer generates descending commands to muscles.\n",
    "\n",
    "2. **Spinal Reflex**:\n",
    "   - Afferent feedback from muscle kinematics (length, velocity, acceleration, jerk).\n",
    "   - Reflex gain applied to the afferent feedback.\n",
    "\n",
    "3. **Muscle Activation Dynamics**:\n",
    "   - Combines descending commands from M1 and spinal reflex.\n",
    "   - Includes time delays for neural and spinal reflex pathways.\n",
    "\n",
    "4. **Musculoskeletal Model**:\n",
    "   - Two-segment arm with six muscles.\n",
    "   - Muscle activations produce forces, which generate joint torques and motion.\n",
    "\n",
    "5. **Feedback Loop**:\n",
    "   - Joint angles, velocities, and muscle states are fed back to the RNNs and spinal reflex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **1. Sensory Layer (`y_s`)**\n",
    "- **Description**: Represents the sensory cortex, which processes sensory feedback (`u_fb`).\n",
    "- **Number of Units**: `n_s = 200` (as per the researchers' description).\n",
    "- **Shape**: `[n_s]` or `[200]` (a 1D tensor with 200 elements).\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Motor Layer (`y_m`)**\n",
    "- **Description**: Represents the motor cortex, which generates descending commands to muscles.\n",
    "- **Number of Units**: `n_m1 = 200` (as per the researchers' description).\n",
    "- **Shape**: `[n_m1]` or `[200]` (a 1D tensor with 200 elements).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Muscle Activation Layer (`y_act`)**\n",
    "- **Description**: Projects the motor cortex output to the muscles. The number of units matches the number of muscles.\n",
    "- **Number of Units**: `n_muscles = 6` (6 muscles in the `myoElbow` model).\n",
    "- **Shape**: `[n_muscles]` or `[6]` (a 1D tensor with 6 elements).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Feedback Input (`u_fb`)**\n",
    "- **Description**: Proprioceptive feedback, including joint angles, velocities, and muscle forces.\n",
    "- **Components**:\n",
    "  - `θ_d(t)`: Delayed joint angles (e.g., shoulder and elbow).\n",
    "  - `θ_g`: Goal joint angles.\n",
    "  - `F_m`: Muscle forces (6 muscles).\n",
    "- **Shape**: `[n_inputs]` or `[10]` (2 joint angles + 2 joint velocities + 6 muscle forces).\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Afferent Feedback (`x_aff`)**\n",
    "- **Description**: Encodes muscle kinematics (length, velocity, acceleration, jerk) for each muscle.\n",
    "- **Components**:\n",
    "  - `Δx_ml`: Muscle length changes.\n",
    "  - `x_mv`: Muscle velocities.\n",
    "  - `x_ma`: Muscle accelerations.\n",
    "  - `x_mj`: Muscle jerks.\n",
    "- **Shape**: `[n_muscles, 4]` or `[6, 4]` (6 muscles × 4 kinematic states).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Weight Matrices**\n",
    "- **Sensory Layer Weights (`w_sr`, `w_si`)**:\n",
    "  - `w_sr`: Recurrent weights for the sensory layer.\n",
    "    - Shape: `[n_s, n_s]` or `[200, 200]`.\n",
    "  - `w_si`: Weights connecting sensory feedback to the sensory layer.\n",
    "    - Shape: `[n_s, n_inputs]` or `[200, 10]`.\n",
    "\n",
    "- **Motor Layer Weights (`w_mr`, `w_mi`)**:\n",
    "  - `w_mr`: Recurrent weights for the motor layer.\n",
    "    - Shape: `[n_m1, n_m1]` or `[200, 200]`.\n",
    "  - `w_mi`: Weights connecting the sensory layer to the motor layer.\n",
    "    - Shape: `[n_m1, n_s]` or `[200, 200]`.\n",
    "\n",
    "- **Muscle Activation Weights (`w_act`)**:\n",
    "  - Weights connecting the motor layer to the muscle activation layer.\n",
    "    - Shape: `[n_muscles, n_m1]` or `[6, 200]`.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Bias Terms**\n",
    "- **Sensory Layer Bias (`b_s`)**:\n",
    "  - Shape: `[n_s]` or `[200]`.\n",
    "- **Motor Layer Bias (`b_m`)**:\n",
    "  - Shape: `[n_m1]` or `[200]`.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Reflex Parameters**\n",
    "- **Stretch Reflex Gain (`G_s`)**:\n",
    "  - Scalar value (e.g., `0.8` or `1.9`).\n",
    "- **Afferent Encoding Gains (`G_aff`)**:\n",
    "  - Shape: `[4]` (for length, velocity, acceleration, jerk).\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Neural Dynamics**\n",
    "- **Sensory Layer Dynamics**:\n",
    "  - `y_s(t+Δt) = (1-(Δt/τ_n))*y_s(t) + (Δt/τ_n) * tanh(u_s(t))`\n",
    "  - Shape of `y_s`: `[n_s]` or `[200]`.\n",
    "- **Motor Layer Dynamics**:\n",
    "  - `y_m(t+Δt) = (1-(Δt/τ_n))*y_m(t) + (Δt/τ_n) * tanh(u_m(t))`\n",
    "  - Shape of `y_m`: `[n_m1]` or `[200]`.\n",
    "- **Muscle Activation Dynamics**:\n",
    "  - `y_act(t+Δt) = (1-(Δt/τ_m))*y_act(t) + (Δt/τ_m) * ReLU(u_act(t))`\n",
    "  - Shape of `y_act`: `[n_muscles]` or `[6]`.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Observation (`obs`)**\n",
    "- **Description**: The observation returned by the environment.\n",
    "- **Components**:\n",
    "  - `qpos`: Joint positions (e.g., shoulder and elbow).\n",
    "  - `qvel`: Joint velocities.\n",
    "  - `act`: Muscle activations.\n",
    "  - `pose_err`: Pose error.\n",
    "- **Shape**: `[11]` (as per the `myoElbow` environment).\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Shapes**\n",
    "\n",
    "| Component                  | Shape         | Description                                      |\n",
    "|----------------------------|---------------|--------------------------------------------------|\n",
    "| `y_s` (Sensory Layer)      | `[200]`       | Sensory cortex output.                          |\n",
    "| `y_m` (Motor Layer)        | `[200]`       | Motor cortex output.                            |\n",
    "| `y_act` (Muscle Activation)| `[6]`         | Muscle activation output.                       |\n",
    "| `u_fb` (Feedback Input)    | `[10]`         | Proprioceptive feedback.                        |\n",
    "| `x_aff` (Afferent Feedback)| `[6, 4]`      | Muscle kinematics (length, velocity, etc.).     |\n",
    "| `w_sr`                    | `[200, 200]`  | Sensory layer recurrent weights.               |\n",
    "| `w_si`                    | `[200, 10]`    | Sensory feedback to sensory layer weights.      |\n",
    "| `w_mr`                    | `[200, 200]`  | Motor layer recurrent weights.                 |\n",
    "| `w_mi`                    | `[200, 200]`  | Sensory layer to motor layer weights.           |\n",
    "| `w_act`                   | `[6, 200]`    | Motor layer to muscle activation weights.       |\n",
    "| `b_s` (Sensory Bias)       | `[200]`       | Sensory layer bias.                             |\n",
    "| `b_m` (Motor Bias)         | `[200]`       | Motor layer bias.                               |\n",
    "| `G_s`                     | `scalar`      | Stretch reflex gain.                            |\n",
    "| `G_aff`                   | `[4]`         | Afferent encoding gains.                        |\n",
    "| `obs`                     | `[11]`         | Observation from the environment.               |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyoSuite:> Registering Myo Envs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "/opt/anaconda3/envs/opensim_scripting/lib/python3.11/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment myoArmReachFixed-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/opt/anaconda3/envs/opensim_scripting/lib/python3.11/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment myoSarcArmReachFixed-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/opt/anaconda3/envs/opensim_scripting/lib/python3.11/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment myoFatiArmReachFixed-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/opt/anaconda3/envs/opensim_scripting/lib/python3.11/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment myoArmReachRandom-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/opt/anaconda3/envs/opensim_scripting/lib/python3.11/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment myoSarcArmReachRandom-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/opt/anaconda3/envs/opensim_scripting/lib/python3.11/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment myoFatiArmReachRandom-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import myosuite\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simulation parameters ---\n",
    "dt = 0.01       # Time step (10 ms)\n",
    "tau_n = 0.02    # Neuron time constant (20 ms)\n",
    "tau_m = 0.05    # Muscle time constant (50 ms)\n",
    "\n",
    "# --- Neural network parameters ---\n",
    "n_s = 200       # Number of sensory neurons\n",
    "n_m1 = 200      # Number of motor neurons\n",
    "n_muscles = 6   # Number of muscles\n",
    "n_inputs = 10   # Size of proprioceptive feedback (from myoElbow observation)\n",
    "\n",
    "# --- Feedback gains ---\n",
    "G_p = 1.0       # Gain for position error\n",
    "G_d = 1.0       # Gain for velocity\n",
    "G_f = 1.0       # Gain for muscle forces\n",
    "\n",
    "# --- Reflex parameters ---\n",
    "G_aff = torch.tensor([1, 0.1, 0.01, 0.0001])  # Encoding gains for length, velocity, acceleration, jerk\n",
    "G_s_low = 0.8                                 # Spinal reflex gain (low reflex gain)\n",
    "G_s_high = 1.9                                # Spinal reflex gain (high reflex gain)\n",
    "reflex_delay = int(0.02 / dt)                 # Spinal reflex delay (20 ms)\n",
    "\n",
    "# --- Transcortical afferent delay ---\n",
    "feedback_delay_steps = int(0.04 / dt)         # Transcortical afferent delay (40 ms)\n",
    "\n",
    "# --- Movement arm matrix (Mm) ---\n",
    "# TODO: Extract these values from myoElbow model directly\n",
    "M_m = torch.tensor([\n",
    "    [0.05, 0.0],  # Muscle 1 (shoulder flexor)\n",
    "    [0.05, 0.0],  # Muscle 2 (shoulder extensor)\n",
    "    [0.0, 0.04],  # Muscle 3 (elbow flexor)\n",
    "    [0.0, 0.04],  # Muscle 4 (elbow extensor)\n",
    "    [0.03, 0.03], # Muscle 5 (biarticular biceps)\n",
    "    [0.03, 0.03]  # Muscle 6 (biarticular triceps)\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# --- Target joint positions ---\n",
    "target_qpos = torch.tensor([1.5, 1.0], dtype=torch.float32)  # Desired joint positions (shoulder, elbow)\n",
    "target_qpos = target_qpos.clone().detach().requires_grad_(True)\n",
    "\n",
    "# --- Penalty coefficients for cost function ---\n",
    "alpha = 0.0001  # Penalizes neural activity\n",
    "beta = 0.01     # Penalizes force output\n",
    "gamma = 0.5     # Penalizes kinematic error\n",
    "\n",
    "# --- Time parameters for background and perturbation loads ---\n",
    "Tb = 200 / 1000    # Background load onset (200 ms)\n",
    "Tp = 1500 / 1000   # Perturbation load onset (1500 ms)\n",
    "T = 3000 / 1000    # Total motion time (3000 ms)\n",
    "Tsb = 1000 / 1000  # Steady-state realization of background load (1000 ms)\n",
    "Tsp = 2500 / 1000  # Steady-state realization of perturbation load (2500 ms)\n",
    "\n",
    "# --- External loads ---\n",
    "background_load_low = 0.1    # Low background load (Nm)\n",
    "background_load_high = 0.55  # High background load (Nm)\n",
    "perturbation_load = 0.25     # Perturbation load (Nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the custom environment\n",
    "from custom_env_2D6M import MyoElbowPose2D6MFixed\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "if \"MyoElbowPose2D6MFixed-v0\" not in gym.envs.registry:\n",
    "    register(\n",
    "        id=\"MyoElbowPose2D6MFixed-v0\",\n",
    "        entry_point=\"custom_env_2D6M:MyoElbowPose2D6MFixed\",\n",
    "    )\n",
    "\n",
    "# Create the custom environment\n",
    "env = gym.make(\"MyoElbowPose2D6MFixed-v0\")\n",
    "\n",
    "# Initialize muscle lengths\n",
    "L_m = torch.ones(n_muscles)\n",
    "\n",
    "# Initialize delayed feedback buffers\n",
    "joint_angle_buffer = [torch.zeros(2) for _ in range(feedback_delay_steps)]\n",
    "joint_velocity_buffer = [torch.zeros(2) for _ in range(feedback_delay_steps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NeuralLimbController\n",
    "class NeuralLimbController(nn.Module):\n",
    "    \"\"\"Neural controller for a two-segment limb model.\"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs=n_muscles):\n",
    "        \"\"\"Initialize the neural controller.\"\"\"\n",
    "        super(NeuralLimbController, self).__init__()\n",
    "        \n",
    "        # Sensory layer (input layer)\n",
    "        self.w_sr = nn.Parameter(torch.empty(n_s, n_s).uniform_(-1/np.sqrt(n_inputs), 1/np.sqrt(n_inputs)))\n",
    "        self.w_si = nn.Parameter(torch.empty(n_s, n_inputs).uniform_(-1/np.sqrt(n_inputs), 1/np.sqrt(n_inputs)))\n",
    "        self.b_s = nn.Parameter(torch.zeros(n_s))\n",
    "\n",
    "        # M1 layer (output layer)\n",
    "        self.w_mr = nn.Parameter(torch.empty(n_m1, n_m1).uniform_(-1/np.sqrt(n_s), 1/np.sqrt(n_s)))\n",
    "        self.w_mi = nn.Parameter(torch.empty(n_m1, n_s).uniform_(-1/np.sqrt(n_s), 1/np.sqrt(n_s)))\n",
    "        self.b_m = nn.Parameter(torch.zeros(n_m1))\n",
    "        \n",
    "        # Muscle activation layer weights (from M1 to muscle)\n",
    "        self.w_act = nn.Parameter(torch.empty(n_muscles, n_m1).uniform_(-1/np.sqrt(n_m1), 1/np.sqrt(n_m1)))\n",
    "        \n",
    "        # Internal state variables\n",
    "        self.y_s = torch.zeros(n_s)          # Sensory Layer output (y_s(t))\n",
    "        self.y_m = torch.zeros(n_m1)         # M1 Layer output (y_m(t))\n",
    "        self.y_act = torch.zeros(n_muscles)  # Muscle Activation output (y_act(t))\n",
    "    \n",
    "    def neural_activation_step(self, y_t, u_t):\n",
    "        \"\"\"Discretized neural activation dynamics.\"\"\"\n",
    "        return (1 - (dt / tau_n)) * y_t + (dt / tau_n) * torch.tanh(u_t)\n",
    "    \n",
    "    def muscle_activation_step(self, y_act_t, u_act_t):\n",
    "        \"\"\"Discretized muscle activation dynamics.\"\"\"\n",
    "        return (1 - (dt / tau_m)) * y_act_t + (dt / tau_m) * torch.relu(u_act_t)\n",
    "\n",
    "    def forward_step(self, u_fb_t, x_aff_t, y_s_delayed_t):\n",
    "        \"\"\"Perform a single forward step of the neural controller.\"\"\"\n",
    "        # Sensory layer input (u_s) and output (y_s)\n",
    "        u_s = self.w_sr @ y_s_delayed_t + self.w_si @ u_fb_t + self.b_s\n",
    "        self.y_s = self.neural_activation_step(self.y_s, u_s)\n",
    "        \n",
    "        # M1 layer input (u_m) and output (y_m)\n",
    "        u_m = self.w_mr @ self.y_m + self.w_mi @ self.y_s + self.b_m\n",
    "        self.y_m = self.neural_activation_step(self.y_m, u_m)\n",
    "        \n",
    "        # Muscle activation layer input (u_act) and output (y_act)\n",
    "        u_act = self.w_act @ self.y_m + G_s_low * x_aff_t\n",
    "        self.y_act = self.muscle_activation_step(self.y_act, u_act)\n",
    "        \n",
    "        return self.y_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the neural controller\n",
    "controller = NeuralLimbController(n_inputs=n_inputs, n_outputs=n_muscles)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = torch.optim.Adam(controller.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/opensim_scripting/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.pose_error to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.pose_error` for environment variables or `env.get_wrapper_attr('pose_error')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/400 | Cumulative Cost: 1908.0212\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     76\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 77\u001b[0m \u001b[43mcost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Generate muscle activations using the neural controller\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/opensim_scripting/lib/python3.11/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/opensim_scripting/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/opensim_scripting/lib/python3.11/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pose_errors = []\n",
    "rewards = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 400\n",
    "for epoch in range(num_epochs):\n",
    "    # Reset environment and variables and initialize the loop\n",
    "    obs, info = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    cumulative_cost = 0\n",
    "    step_count = 0\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        # Reinitialize the controller's internal state\n",
    "        controller.y_s = torch.zeros_like(controller.y_s, requires_grad=True)\n",
    "        controller.y_m = torch.zeros_like(controller.y_m, requires_grad=True)\n",
    "        controller.y_act = torch.zeros_like(controller.y_act, requires_grad=True)\n",
    "\n",
    "        # Extract observation components\n",
    "        qpos = torch.tensor(obs[:2], dtype=torch.float32)  # Joint positions (shoulder, elbow)\n",
    "        qvel = torch.tensor(obs[2:4], dtype=torch.float32)  # Joint velocities (shoulder, elbow)\n",
    "        act = torch.tensor(obs[4:10], dtype=torch.float32, requires_grad=True)  # Muscle activations (6 muscles)\n",
    "        pose_err = torch.tensor(obs[10], dtype=torch.float32)  # Pose error (scalar)\n",
    "\n",
    "        # Compute muscle length changes\n",
    "        dLm_dt = (-M_m @ qvel.unsqueeze(1)).squeeze()\n",
    "        L_m = L_m + dLm_dt.squeeze() * dt\n",
    "\n",
    "        # Ensure L_m and dLm_dt require gradients\n",
    "        L_m = L_m.clone().detach().requires_grad_(True)\n",
    "        dLm_dt = dLm_dt.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # Update delayed feedback buffers\n",
    "        joint_angle_buffer.append(qpos)\n",
    "        joint_velocity_buffer.append(qvel)\n",
    "        delayed_qpos = joint_angle_buffer.pop(0)\n",
    "        delayed_qvel = joint_velocity_buffer.pop(0)\n",
    "\n",
    "        # Compute muscle forces (F_m) using FL and FV properties\n",
    "        # TODO: Adjust FL and FV to match MyoSuite's muscle model\n",
    "        FL = torch.clamp(1.0 - (L_m - 1.0).pow(2), min=0.0)\n",
    "        FV = torch.clamp(1.0 - dLm_dt.abs(), min=0.0)\n",
    "        F_m = act * FL * FV\n",
    "\n",
    "        # Compute feedback input (u_fb)\n",
    "        position_error = G_p * (delayed_qpos - target_qpos)\n",
    "        velocity_feedback = G_d * delayed_qvel\n",
    "        force_feedback = G_f * F_m\n",
    "        u_fb = torch.cat([position_error, velocity_feedback, force_feedback.view(-1)])\n",
    "\n",
    "        # Compute neural activity cost\n",
    "        neural_cost = controller.y_s.pow(2).mean() + controller.y_m.pow(2).mean()\n",
    "\n",
    "        # Compute force output cost\n",
    "        force_cost = F_m.pow(2).mean()\n",
    "\n",
    "        # Background load kinematic cost\n",
    "        J_b_1 = (qpos - target_qpos).pow(2).mean() + 0.5 * qvel.pow(2).mean()  # [0, T_b]\n",
    "        J_b_2 = (qpos - target_qpos).pow(2).mean() + 0.5 * qvel.pow(2).mean()  # [T_sb, T_p]\n",
    "        J_b = (J_b_1 / Tb) + (J_b_2 / (Tp - Tsb))\n",
    "\n",
    "        # Perturbation load kinematic cost\n",
    "        J_p = (qpos - target_qpos).pow(2).mean() + 0.5 * qvel.pow(2).mean()    # [T_sp, T]\n",
    "        J_p /= (T - Tsp)\n",
    "\n",
    "        # Compute kinematic cost\n",
    "        kinematic_cost = J_b + J_p\n",
    "\n",
    "        # Composite cost function\n",
    "        cost = alpha * neural_cost + beta * force_cost + gamma * kinematic_cost\n",
    "        cumulative_cost += cost.detach().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Generate muscle activations using the neural controller\n",
    "        x_aff_t = torch.zeros(n_muscles)  # Placeholder for afferent feedback\n",
    "        muscle_activations = controller.forward_step(\n",
    "            u_fb_t=u_fb,                  # Feedback input\n",
    "            x_aff_t=x_aff_t,              # Afferent feedback\n",
    "            y_s_delayed_t=controller.y_s  # Delayed sensory layer output\n",
    "        )\n",
    "\n",
    "        # Step the environment\n",
    "        obs, reward, terminated, truncated, info = env.step(muscle_activations.detach().numpy())\n",
    "\n",
    "        pose_errors.append(env.pose_error)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        step_count += 1\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Cumulative Cost: {cumulative_cost:.4f}\")\n",
    "\n",
    "# Plot the pose error\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(pose_errors, label=\"Pose Error\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Pose Error\")\n",
    "plt.title(\"Pose Error Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards, label=\"Reward\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "# from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# # Wrap the MyoElbow environment\n",
    "# env = gym.make(\"myoElbowPose1D6MFixed-v0\")\n",
    "\n",
    "# # Wrap the environment for vectorized training\n",
    "# vec_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "# # Define the PPO model\n",
    "# model = PPO(\"MlpPolicy\", vec_env, verbose=1, learning_rate=0.001, n_steps=2048)\n",
    "\n",
    "# # Train the model\n",
    "# model.learn(total_timesteps=100000)\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save(\"ppo_myo_elbow\")\n",
    "\n",
    "# # Evaluate the trained model\n",
    "# obs, info = env.reset()\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action, _ = model.predict(obs)\n",
    "#     obs, reward, done, truncated, info = env.step(action)\n",
    "#     print(f\"Reward: {reward}\")\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomPolicy(ActorCriticPolicy):\n",
    "#     def __init__(self, observation_space, action_space, lr_schedule, **kwargs):\n",
    "#         super(CustomPolicy, self).__init__(observation_space, action_space, lr_schedule, **kwargs)\n",
    "\n",
    "#         # Use your NeuralLimbController as the policy network\n",
    "#         self.controller = NeuralLimbController(n_inputs=n_inputs, n_outputs=n_muscles)\n",
    "\n",
    "#         # Define the value network (critic)\n",
    "#         self.value_net = nn.Sequential(\n",
    "#             nn.Linear(n_inputs, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, obs: torch.Tensor):\n",
    "#         # Process the observation and generate muscle activations\n",
    "#         u_fb_t = obs[:, :n_inputs]  # Extract proprioceptive feedback\n",
    "#         x_aff_t = obs[:, n_inputs:]  # Extract afferent feedback\n",
    "#         y_s_delayed_t = torch.zeros(n_s)  # Placeholder for delayed sensory layer output\n",
    "\n",
    "#         # Generate muscle activations (actions)\n",
    "#         actions = self.controller.forward_step(u_fb_t, x_aff_t, y_s_delayed_t)\n",
    "\n",
    "#         # Compute value estimate\n",
    "#         values = self.value_net(obs)\n",
    "\n",
    "#         return actions, values\n",
    "\n",
    "#     def _predict(self, observation: torch.Tensor, deterministic: bool = False):\n",
    "#         # Predict actions (muscle activations)\n",
    "#         actions, _ = self.forward(observation)\n",
    "#         return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import PPO\n",
    "\n",
    "# # Wrap the environment\n",
    "# env = gym.make(\"myoElbowPose1D6MFixed-v0\")\n",
    "\n",
    "# # Define the PPO model with the custom policy\n",
    "# model = PPO(CustomPolicy, env, verbose=1, learning_rate=0.001, n_steps=2048)\n",
    "\n",
    "# # Train the model\n",
    "# model.learn(total_timesteps=100000)\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save(\"ppo_myo_elbow_custom_policy\")\n",
    "# print(\"Model saved as 'ppo_myo_elbow_custom_policy'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opensim_scripting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
